---
slug: llm
toAstro: true
description: >-
  本文介绍了大型语言模型（LLM）中的`temperature`参数如何影响模型的输出结果。当`temperature`值较低时，模型输出更确定的结果；而当该值较高时，模型可能产生更多样化或更具创造性的输出。此外，文章还提供了学习与AI沟通的资源链接，包括Learn
  Prompting和Prompt Engineering Guide。
tags:
  - LLM
  - temperature
  - random，prompt engineering，生成型语言模型
date_created: 2025-01-04T03:34:08.000Z
date_modified: 2025-02-19T03:44:10.000Z
title: LLM
---

# LLM

TODO

`temperature`  的参数值越小，模型就会返回越确定的一个结果。如果调高该参数值，大语言模型可能会返回更随机的结果，也就是说这可能会带来更多样化或更具创造性的产出。

[欢迎 | Learn Prompting: Your Guide to Communicating with AI](<https://learnprompting.org/zh-Hans/docs/intro>)  
[提示工程指南 | Prompt Engineering Guide<!-- -->](<https://www.promptingguide.ai/zh>)

arxiv.org/abs/2201.11903
